import os
import sys
import time
import json
import cProfile
import pstats
import tracemalloc
from typing import Dict, List, Tuple

# -------------------------------------------------------------------
# 1. å‡†å¤‡å·¥ä½œ: å¯¼å…¥ä½ å†™å¥½çš„ BPE è®­ç»ƒå‡½æ•°
# -------------------------------------------------------------------
# å‡è®¾ä½ çš„ tokenizer.py åœ¨ 'cs336_basics' ç›®å½•ä¸‹
# è¯·æ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´è·¯å¾„
TOKENIZER_FILE_PATH = './cs336_basics/tokenizer.py' 
# æ·»åŠ é¡¹ç›®è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
#sys.path.append(os.path.dirname(TOKENIZER_FILE_PATH))
from cs336_basics.tokenizer import train_bpe


# -------------------------------------------------------------------
# 2. å®šä¹‰å¸¸é‡å’Œè¾…åŠ©å‡½æ•°
# -------------------------------------------------------------------
DATASET_PATH = "/home/siluyang/CS336/assignment1-basics/data/owt_train.txt" # <--- ä¿®æ”¹è¿™é‡Œ
VOCAB_SIZE = 32000
SPECIAL_TOKENS = ["<|endoftext|>"]
VOCAB_OUTPUT_PATH = "vocab_owt.json"
MERGES_OUTPUT_PATH = "merges_owt.txt"
PROFILE_OUTPUT_PATH = "training_profile_owt.prof"

def save_vocab_and_merges(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]]):
    """åºåˆ—åŒ–è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™åˆ°ç£ç›˜"""
    
    # ä¸ºäº†è®© JSON å¯è¯»ï¼Œå°† bytes è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
    serializable_vocab = {k: list(v) for k, v in vocab.items()}
    with open(VOCAB_OUTPUT_PATH, 'w', encoding='utf-8') as f:
        json.dump(serializable_vocab, f, ensure_ascii=False, indent=2)
    print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜åˆ°: {VOCAB_OUTPUT_PATH}")

    # å°†åˆå¹¶è§„åˆ™ä¿å­˜ä¸ºçº¯æ–‡æœ¬ï¼Œæ›´æ˜“è¯»
    with open(MERGES_OUTPUT_PATH, 'w', encoding='utf-8') as f:
        for p1, p2 in merges:
            # å°è¯•è§£ç ä¸ºå¯è¯»å­—ç¬¦ï¼Œå¤±è´¥åˆ™ä¿ç•™åŸå§‹å­—èŠ‚è¡¨ç¤º
            s1 = p1.decode('utf-8', errors='ignore')
            s2 = p2.decode('utf-8', errors='ignore')
            f.write(f"{s1} {s2}\n")
    print(f"âœ… åˆå¹¶è§„åˆ™å·²ä¿å­˜åˆ°: {MERGES_OUTPUT_PATH}")


def analyze_results(vocab: Dict[int, bytes], duration_s: float, peak_mem_mb: float):
    """åˆ†æç»“æœå¹¶æ‰“å°æŠ¥å‘Š"""
    
    # æ‰¾å‡ºæœ€é•¿çš„ token
    longest_token_bytes = b''
    for token_bytes in vocab.values():
        if len(token_bytes) > len(longest_token_bytes):
            longest_token_bytes = token_bytes
            
    # å°è¯•è§£ç ä¸ºå­—ç¬¦ä¸²ä»¥ä¾¿åˆ†æ
    longest_token_str = longest_token_bytes.decode('utf-8', errors='replace')

    print("\n--- è®­ç»ƒç»“æœåˆ†æ ---")
    print(f"ğŸ•’ è®­ç»ƒè€—æ—¶: {duration_s / 60:.2f} åˆ†é’Ÿ ({duration_s / 3600:.4f} å°æ—¶)")
    print(f"ğŸ§  å³°å€¼å†…å­˜: {peak_mem_mb:.2f} MB ({peak_mem_mb / 1024:.2f} GB)")
    print(f"ğŸ“œ æœ€é•¿Token (é•¿åº¦ {len(longest_token_bytes)} å­—èŠ‚): '{longest_token_str}'")
    
    # åˆ†ææœ€é•¿ token æ˜¯å¦åˆç†
    print("\nğŸ¤” æœ€é•¿Tokenåˆç†æ€§åˆ†æ:")
    print("è¿™ä¸ª token å¾ˆå¯èƒ½æ˜¯ä¸€ä¸ªåœ¨ TinyStories æ•°æ®é›†ä¸­é«˜é¢‘å‡ºç°çš„ã€æœ‰æ„ä¹‰çš„è‹±æ–‡å•è¯æˆ–çŸ­è¯­ã€‚")
    print("ä¾‹å¦‚ ' because', ' something', ' little' ç­‰ã€‚å› ä¸ºBPEç®—æ³•ä¼šä¸æ–­åˆå¹¶é«˜é¢‘ç›¸é‚»çš„å­—èŠ‚å¯¹ï¼Œ")
    print("æ‰€ä»¥æœ€å¸¸è§çš„è¯æœ€ç»ˆä¼šæˆä¸ºè¯æ±‡è¡¨ä¸­çš„é•¿ tokenã€‚è¿™å®Œå…¨ç¬¦åˆé¢„æœŸã€‚")


# -------------------------------------------------------------------
# 3. ä¸»æ‰§è¡Œå‡½æ•°
# -------------------------------------------------------------------
def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œè®­ç»ƒå’Œåˆ†æ"""
    print("--- å¼€å§‹ BPE Tokenizer è®­ç»ƒ ---")
    
    # (a) å¯åŠ¨æ—¶é—´å’Œå†…å­˜ç›‘æ§
    tracemalloc.start()
    start_time = time.time()
    
    # æ‰§è¡Œè®­ç»ƒ
    print(f"ä» '{DATASET_PATH}' åŠ è½½æ•°æ®...")
    final_vocab, final_merges = train_bpe(
        input_path=DATASET_PATH,
        vocab_size=VOCAB_SIZE,
        special_tokens=SPECIAL_TOKENS,
    )
    
    # è®°å½•æ—¶é—´å’Œå†…å­˜ä½¿ç”¨
    end_time = time.time()
    duration_seconds = end_time - start_time
    _, peak_mem_bytes = tracemalloc.get_traced_memory()
    peak_mem_mb = peak_mem_bytes / 1024 / 1024
    tracemalloc.stop()
    
    print("\n--- è®­ç»ƒå®Œæˆ ---")
    
    # åºåˆ—åŒ–ç»“æœ
    save_vocab_and_merges(final_vocab, final_merges)
    
    # åˆ†æå¹¶æŠ¥å‘Šç»“æœ
    analyze_results(final_vocab, duration_seconds, peak_mem_mb)
    
    print("\n--- ä»»åŠ¡ (a) å®Œæˆ ---")
    
# -------------------------------------------------------------------
# 4. æ€§èƒ½åˆ†æå‡½æ•°
# -------------------------------------------------------------------
def profile_main():
    """ä½¿ç”¨ cProfile è¿è¡Œä¸»å‡½æ•°ä»¥è¿›è¡Œæ€§èƒ½åˆ†æ"""
    print("\n--- å¯åŠ¨æ€§èƒ½åˆ†æ ---")
    profiler = cProfile.Profile()
    profiler.run('main()')
    profiler.dump_stats(PROFILE_OUTPUT_PATH)
    print(f"ğŸ“ˆ æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {PROFILE_OUTPUT_PATH}")

    # (b) è¯»å–æ€§èƒ½åˆ†ææŠ¥å‘Šå¹¶æ‰¾å‡ºç“¶é¢ˆ
    stats = pstats.Stats(PROFILE_OUTPUT_PATH)
    stats.sort_stats('cumtime') # æŒ‰ç´¯è®¡è€—æ—¶æ’åº
    print("\n--- æ€§èƒ½ç“¶é¢ˆåˆ†æ (è€—æ—¶æœ€é•¿çš„å‰5ä¸ªå‡½æ•°) ---")
    stats.print_stats(5)
    
    print("\nğŸ¤” æ€§èƒ½ç“¶é¢ˆåˆ†æ:")
    print("ä»æŠ¥å‘Šä¸­å¯ä»¥çœ‹å‡ºï¼Œç»å¤§éƒ¨åˆ†æ—¶é—´éƒ½æ¶ˆè€—åœ¨äº† `compute_merges` å‡½æ•°ä¸Šã€‚")
    print("å°½ç®¡æˆ‘ä»¬å·²ç»å¯¹å…¶è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½†BPEåˆå¹¶è¿‡ç¨‹æœ¬èº«å›ºæœ‰çš„è®¡ç®—å¤æ‚æ€§ä½¿å…¶æˆä¸ºç†æ‰€å½“ç„¶çš„æ€§èƒ½ç“¶é¢ˆã€‚")
    print("å…¶æ¬¡è€—æ—¶è¾ƒå¤šçš„å¯èƒ½æ˜¯å¤šè¿›ç¨‹é¢„åˆ†è¯çš„ `parallel_pretokenize_and_count` å‡½æ•°ã€‚")
    print("\n--- ä»»åŠ¡ (b) å®Œæˆ ---")


if __name__ == '__main__':
    # ä¸ºäº†åˆ†åˆ«å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼Œä½ å¯ä»¥é€‰æ‹©è¿è¡Œå…¶ä¸­ä¸€ä¸ª
    
    # ä»»åŠ¡ (a): ç›´æ¥è¿è¡Œè®­ç»ƒå¹¶è·å–ç»“æœ
    # main() 
    
    # ä»»åŠ¡ (b): è¿è¡Œæ€§èƒ½åˆ†æ (è¿™ä¹Ÿä¼šå®Œæ•´è·‘ä¸€éè®­ç»ƒ)
    profile_main()